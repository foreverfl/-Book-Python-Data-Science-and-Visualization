# [학습자료] 데이터 사이언스와 시각화

## 목차

1. 데이터 사이언스 개요\_데이터 분석 맛보기
2. [데이터 사이언스 개요\_분석 환경 준비하기](#2-데이터-사이언스-개요_분석-환경-준비하기)
3. [데이터 조작 이해와 실무\_데이터 선택](#3-데이터-조작-이해와-실무\_데이터-선택)
4. [데이터 조작 이해와 실무\_데이터 변경](#4-데이터-조작-이해와-실무\_데이터-변경)
5. [데이터 조작 이해와 실무\_데이터 합치기](#5-데이터-조작-이해와-실무\_데이터-합치기)
6. [데이터 조작 이해와 실무\_데이터 그룹핑](#6-데이터-조작-이해와-실무\_데이터-그룹핑)
7. [데이터 조작 이해와 실무\_시계열 데이터 기초](#7-데이터-조작-이해와-실무\_시계열-데이터-기초)
8. [데이터 전처리 이해와 실무\_데이터 정제 – 결측 데이터 처리](#8-데이터-전처리-이해와-실무\_데이터-정제---결측-데이터-처리)
9. [데이터 전처리 이해와 실무\_데이터 정체 – 이상 데이터 처리](#9-데이터-전처리-이해와-실무\_데이터-정체---이상-데이터-처리)
10. [데이터 전처리 이해와 실무\_데이터 변환 – 정규화, 구간화](#10-데이터-전처리-이해와-실무\_데이터-변환---정규화,-구간화)
11. [데이터 전처리 이해와 실무\_데이터 변환 – 특징 생성](#11-데이터-전처리-이해와-실무\_데이터-변환---특징-생성)
12. [데이터 전처리 이해와 실무\_데이터 축소 – 특징 선택](#12-데이터-전처리-이해와-실무\_데이터-축소---특징-선택)
13. [데이터 탐색 이해와 실무\_일변량 비시각화 탐색](#13-데이터-탐색-이해와-실무\_일변량-비시각화-탐색)
14. [데이터 탐색 이해와 실무\_일변량 시각화 탐색](#14-데이터-탐색-이해와-실무\_일변량-시각화-탐색)
15. [데이터 탐색 이해와 실무\_다변량 비시각화 탐색](#15-데이터-탐색-이해와-실무\_다변량-비시각화-탐색)
16. [데이터 탐색 이해와 실무\_다변량 시각화 탐색](#16-데이터-탐색-이해와-실무\_다변량-시각화-탐색)
17. [데이터 분석 이해와 실무\_가설의 의의와 검정](#17-데이터-분석-이해와-실무\_가설의-의의와-검정)
18. [데이터 분석 이해와 실무\_상관 분석](#18-데이터-분석-이해와-실무\_상관-분석)
19. [데이터 분석 이해와 실무\_회귀 분석](#19-데이터-분석-이해와-실무\_회귀-분석)
20. [데이터 분석 이해와 실무\_시계열 분석](#20-데이터-분석-이해와-실무\_시계열-분석)
21. [데이터 시각화 이해와 실무\_정적 시각화 개요](#21-데이터-시각화-이해와-실무\_정적-시각화-개요)
22. [데이터 시각화 이해와 실무\_정적 시각화 실습 – matplotlib 활용](#22-데이터-시각화-이해와-실무\_정적-시각화-실습---matplotlib-활용)
23. [데이터 시각화 이해와 실무\_정적 시각화 실습 – seaborn 활용](#23-데이터-시각화-이해와-실무\_정적-시각화-실습---seaborn-활용)
24. [데이터 시각화 이해와 실무\_동적 시각화 개요](#24-데이터-시각화-이해와-실무\_동적-시각화-개요)
25. [데이터 시각화 이해와 실무\_동적 시각화 실습 – plotly 활용](#25-데이터-시각화-이해와-실무\_동적-시각화-실습---plotly-활용)
26. [데이터 시각화 이해와 실무\_UI 시각화 실습 – streamlit 활용](#26-데이터-시각화-이해와-실무\_UI-시각화-실습---streamlit-활용)


## 2. 데이터 사이언스 개요\_분석 환경 준비하기

### 특정 파이썬 버전으로 새로운 conda 가상 환경 생성
> ```shell
> conda create --name [ENV_NAME] python=[PYTHON_VERSION]
> ```

### 모든 conda 환경들을 나열
> ```shell
> conda info --envs
> ```

### conda 환경 활성화 / 현재 conda 환경 비활성화
> ```shell
> conda activate [ENV_NAME]
> conda deactivate
> ```

### 현재 conda 환경에 Jupyter Notebook 설치
> ```shell
> conda install jupyter notebook
> ```

### 현재 conda 환경에 특정 패키지 설치
> ```shell
> conda install [PACKAGE_NAME]
> ```

### conda 환경을 Jupyter Notebook에 등록
> ```shell
> python -m ipykernel install --user --name [ENV_NAME] --display-name "[DISPLAY_NAME]"
> ```

### Jupyter Notebook 시작
> ```shell
> jupyter notebook
> ```

### 기존 환경을 복제하여 새로운 conda 환경 생성
> ```shell
> conda create --name [NEW_ENV_NAME] --clone [EXISTING_ENV_NAME]
> ```

### conda 환경 제거
> ```shell
> conda env remove --name [ENV_NAME]
> ```

### 현재 conda 환경을 YAML 파일로 내보내기

> ```shell
> conda env export > [ENV_NAME].yaml
> ```

### YAML 파일로부터 새로운 conda 환경 생성

> ```shell
> conda env create --file [ENV_NAME].yaml
> ```

## 3. 데이터 조작 이해와 실무\_데이터 선택
### Numpy: Python에서 수치 계산을 위한 핵심 라이브러리. 대규모 다차원 배열 및 행렬 연산에 필요한 다양한 함수를 제공하며, 이는 고성능의 수학 연산과 과학 연산 작업에 필수적.
> #### 주요 특징
> 1. 다차원 배열 객체: NumPy의 핵심 기능은 ndarray라는 다차원 배열 객체.
> 2. 브로드캐스팅: 다른 크기의 배열 간 연산을 지원하는 기능.
> 3. 통합 수학 함수: 배열 데이터를 처리하는 데 필요한 기본적인 수학 함수를 제공.
> 4. 선형 대수, 난수 생성, 푸리에 변환과 같은 고급 수학 및 통계 연산 지원.
> 5. C, C++, 포트란과 같은 저수준 언어로 작성된 코드와 통합이 가능.

### Pandas: Python에서 데이터 분석을 위한 가장 인기 있는 라이브러리. 고성능의 DataFrame 객체를 제공하여 데이터 분석, 클린징, 처리, 시각화 등의 작업을 효과적으로 수행 가능.
> #### 주요 특징
> 1. 데이터 구조: Series와 DataFrame이라는 두 가지 주요 데이터 구조를 제공.
> 2. Series: 1차원 배열과 같은 데이터 구조로, 여러 데이터 타입을 저장할 수 있음.
> 3. DataFrame: 2차원 레이블이 있는 데이터 구조로, 다양한 타입의 열을 가질 수 있음.
> 4. 데이터 처리: 데이터 병합, 조인, 리쉐이핑, 피벗, 슬라이싱, 인덱싱, 데이터 클린징 등의 다양한 데이터 처리 기능을 제공.
> 5. 데이터 분석: 그룹화, 집계, 통계 계산 등의 기능을 통해 데이터 분석을 지원.
> 6. 시계열 처리: 날짜 및 시간 데이터를 위한 기능을 제공하며, 시계열 분석 및 빈도 변환 등의 작업을 수행 가능.
> 7. 데이터 입출력: CSV, Excel, SQL, HDF5, Parquet 등 다양한 데이터 소스로부터 데이터를 읽고 쓸 수 있는 기능을 제공.

## 4. 데이터 조작 이해와 실무\_데이터 변경
### 주어진 data로부터 데이터프레임을 생성
> ```python
> pandas.DataFrame(data):
> ```

### 주어진 데이터프레임에 다른 데이터프레임, 시리즈, 딕셔너리, 배열을 추가
> ```python
> DataFrame.append(data, ignore_index=True):
> ```

### 특정 인덱스의 특정 열 값에 빠르게 접근
> ```python
> DataFrame.at[index, 'column_name']:
> ```

### 특정 인덱스(행)을 제거한 새로운 데이터프레임을 반환
> ```python
> DataFrame.drop(index)
> ```

### 특정 열을 제거한 새로운 데이터프레임을 반환
> ```python
> DataFrame.drop(columns=column_name)
> ```

### 특정 열을 데이터프레임의 인덱스로 설정
> ```python
> DataFrame.set_index(column_name, drop=True)
> ```

## 5. 데이터 조작 이해와 실무\_데이터 합치기
### 데이터 병합: 각기 다른 두 개 이상의 Dataframe을 하나로 병합하여 결과 집합을 만들어 내는 것.
> - Inner Join: Dataframe 간 조인 조건을 만족하는 행을 합치는 것.
> 1. One-to-One
> 2. One-to-Many
> - Outer Join: 조건에 부합하지 않는 행까지 포함시켜 결합하는 방법. NaN 처리를 함.
> - Left Join: 첫 번째 Dataframe을 기준으로 두 번째 Dataframe을 결합하는 방법.
> - Right Join: 첫 번째 Dataframe을 기준으로 첫 번째 Dataframe을 결합하는 방법.

### Join
> - 인덱스를 기준으로 함.
> - DataFrame의 메소드로 사용함.
> - 왼쪽 DataFrame을 기준으로함.
> - 중복된 칼럼이 있다면, 두 개 칼럼 모두 Dataframe에 다른 이름올 저장됨.

### Merge
> - 열을 기준으로 함.
> - Pandas의 함수로 사용함.
> - 기본적으로 inner join. how 파라미터를 통해 변경 가능.

### Concatenate
> - 연결은 공유하는 Key 값을 사용하지 않고, 데이터를 기존 DataFrame 아래(또는 우측)에 붙여 연결함.

## 6. 데이터 조작 이해와 실무\_데이터 그룹핑
### groupby: 특정 열, 열의 리스트 또는 행 인덱스를 기준으로 그룹화를 진행함.
> - count: 각 그룹의 각 열의 누락되지 않은 값의 개수를 세어줌.
> - sum: 각 그룹의 각 열의 합계를 구해줌.
> - mean: 각 그룹의 각 열의 평균값을 구해줌.
> - median: 각 그룹의 각 열의 중앙값을 구해줌.
> - min: 각 그룹의 각 열의 최솟값을 구해줌.
> - max: 각 그룹의 각 열의 최댓값을 구해줌.
> - var: 각 그룹의 각 열의 분산을 구해줌.
> - std: 각 그룹의 각 열의 표준편차를 구해줌.
> - first: 각 그룹의 각 열의 첫 번째 값 (누락되지 않은)을 가져옴.
> - last: 각 그룹의 각 열의 마지막 값 (누락되지 않은)을 가져옴.
> - describe: 각 그룹의 각 열의 기술통계량을 계산하여 반환함. (평균, 표준편차, 최소값, 최대값 등)
> - aggregate 또는 agg: 사용자가 지정한 하나 이상의 연산을 각 그룹의 각 열에 적용함. 집계 함수의 리스트나 딕셔너리를 전달할 수 있음.
> - apply: 각 그룹의 각 열에 함수를 적용하고, 결과를 합쳐줌. 매우 유연하며 사용자 정의 함수를 사용할 수 있음.

### sort_values
> - 형태
> ```python
> dataframe.sort_values(by='열이름')
> ```
> 
> - 옵션
> 1. by: 정렬할 기준이 되는 열의 이름 또는 열의 이름 리스트를 지정. 여러 열을 리스트로 전달하면, 순차적으로 정렬됨.
> 2. axis: 정렬을 수행할 축을 지정. 0은 행 기준, 1은 열 기준. 기본값은 0입니다.
> 3. ascending: 정렬 순서를 지정. True이면 오름차순, False이면 내림차순. 여러 열을 정렬할 때는 불리언 값의 리스트를 전달할 수 있음. 기본값은 True.
> 4. inplace: 원본 객체를 변경할지 여부를 지정. True이면 원본 객체가 변경되고, False이면 새로운 객체가 반환됨. 기본값은 False.
> 5. na_position: 결측값(NaN)의 위치를 지정. 'first'이면 결측값이 처음에 위치하고, 'last'이면 마지막에 위치합. 기본값은 'last'.
> 6. ignore_index: 인덱스를 재설정할지 여부를 지정. True이면 인덱스가 재설정되고, False이면 원본 인덱스가 유지됨. 기본값은 False.

## 7. 데이터 조작 이해와 실무\_시계열 데이터 기초
### - 시계열 데이터: 순차적인 시간의 흐름으로 기록된 관측치의 집합
> - 시간이 순차적인 경우에 사용.
> - 고정된 시간 구간의 관측치: 시간 구간이 일정해야함.
> - DatetimeIndex라는 자료형을 사용함.

### 시간 그래프(Time Plot): 패턴, 이상치, 시간에 따른 변화, 계절성 등의 데이터의 많은 특징을 눈으로 볼 수 있게 해줌.

### 문자열을 datetime으로 변환
> ```python
> pandas.to_datetime() 
> ```

### 데이터를 특정 방향으로 이동시키는 기능을 제공.
> ```python
> shift()
> ```

### 시계열 데이터에서 각 요소 간의 차이를 계산하는 기능을 제공
> ```python
> diff()
> ```

### Resampling
> #### 다운샘플링: 원래의 데이터가 그룹으로 묶여 대표값 필요. 데이터의 양이 감소.
> #### 업샘플링: 실제로 존재하지 않는 데이터를 만듦. 데이터의 값이 증가.
> ##### Forward filling: 처음 데이터를 기준으로 결측치를 보관함.
> - 시계열 데이터에서 값이 연속적으로 유지되어야 할 때.
> - 센서로부터의 읽기 누락 등이 있어 직전 데이터를 현재 값으로 사용해야 할 때.
> 
> ##### Backward filling: 마지막 데이터를 기준으로 결측치를 보관함.
> - 미래의 데이터가 과거의 누락된 값에 더 영향을 미칠 것으로 예상되는 상황.
> - 특정 이벤트 후의 데이터가 더 중요한 정보를 제공할 수 있을 때.

## 8. 데이터 전처리 이해와 실무\_데이터 정제 - 결측 데이터 처리
> ### - 제거하기: 데이터 손실 발생.
> - Likewise deletion: 결측치가 존재하는 행 삭제.
> - Pairwise deletion: 모든 변수가 결측치로만 존재하는 행 삭제.

### 대체하기: 편향(Bias) 발생 가능. 정보의 손실을 방지하나 변수 특성(평균, 상관관계)에 영향 발생.
> - 일정 값 대체
> - 선형 값 대체
> 1. linear(기본값): 선형 보간을 수행하여 결측값을 주변 두 값의 선형 비율에 따라 추정.
> 2. time: 시계열 데이터에서 사용할 때, 시간에 따른 선형 보간을 수행.
> 3. polynomial: 다항식 보간을 수행.
> 4. spline: 스플라인(spline) 보간을 수행.

## 9. 데이터 전처리 이해와 실무\_데이터 정체 - 이상 데이터 처리
### Z-score
> - 데이터가 평균에서 얼마나 떨어져 있는지를 표준편차의 단위로 나타내는 값.
> - 평균으로부터 얼마나 떨어져 있는지를 알 수 있으므로 데이터가 평균으로부터 얼마나 표준편차만큼 떨어져 있는지를 측정하는 표준화된 값으로 사용됨.
> - Z = (X - μ) / σ
> 1. X: 개별 데이터 포인트 값.
> 2. μ: 데이터의 평균.
> 3. σ: 데이터의 표준편차.

### IQR(Interquartile Range)
> - 데이터의 중간 50% 범위를 나타내는 값.
> - 데이터의 중간 50% 범위를 간단하게 측정하는 방법.
> - IQR = Q3 - Q1
> 1. Q3: 75% 지점
> 2. Q1: 25% 지점

## 10. 데이터 전처리 이해와 실무\_데이터 변환 - 정규화, 구간화
### Transformation: 여러 형태로 표현된 데이터 값을 다양한 분석 방법론에 적용하기 위해 원시 형태에서 다른 형식으로 바꾸는 과정
> - 목적 및 특징
> 1. 빠른 특성 파악
> 2. 분석 알고리즘 적용: 다른 범위를 지닌 변수들의 변환 필요

### 구간화(Binning): 데이터를 더 쉽게 이해하고 분석할 수 있게 도와줌. 연속형 데이터를 구간으로 구별하여 범주화 형태 변환.
> - 특징
> 1. 지정 길이 기반 구간 정의: 직접 구간의 크기를 정해서 나누는 방법. 사용자 기준으로 데이터 범위의 간격을 구분하여 관측치를 나눔.
> 2. 분포 기간 구간 정의: 모든 구간에 같은 개수의 숫자가 들어가도록 나누는 것. 관측치가 각 구간 내 동일한 개수로 구분되도록 나눔.

### 정규화: 데이터 탐색 및 기계학습 적용을 위한 연속형 변수 변환.
> - 최대-최소 정규화
> 1. 데이터 구간을 0에서 1사이로 변환. 데이터의 위치 파악.
> 2. 이상치에 민감할 수 있는 단점.
> 3. X(normalized) = (X - X(min)) / (X(max) - X(min))
> 
> - Z-점수 정규화(표준화)
> 1. 0을 중심으로 양쪽으로 데이터 분포시킴.
> 2. 특정 데이터가 평균화 얼마나 떨어져 있는지 파악.
> 3. Z = (X - μ(평균)) / a(표준편차)

## 11. 데이터 전처리 이해와 실무\_데이터 변환 - 특징 생성
### Feature Creation: 원본 데이터의 조합/변환 등을 기반하여 새로운 특징들을 구축 및 생성하는 방법

### 목적 및 필요성
> - 품질 확보: 가공을 거치지 않은 Raw 데이터 활용 기반의 모델링은 품질의 확보가 어려움.
> - 최적화된 형태 변환: 효과적인 Feature를 확보하는 것이 데이터 분석에서 가장 중요함.

### 차원 축소: 원본 데이터로부터 새로운 특징의 집합을 생성. 고차원 원시 데이터 셋을 저차원으로 축소.
> - 범주 인코딩: 크게 Nominal(순서가 없는)과 Ordinal(순서가 있는)으로 나뉘는 범주형 변수. 숫자가 아닌 범주 변수 값을 숫자로 표현.
> * One-hot Encoding: Nominal한 변수를 처리. K개의 범주를 지닌 범주형 변수를 K개의 변수로 변환.

### 결합 및 분해: 데이터 셋 변수들의 조합을 기반으로 새로운 특징을 구축. 변수간의 연산 혹은 분해를 통해 새로운 특징을 구축.
> - 결합
> 1. Add/Divide: 종합적 수치 파악.
> 2. Subtract: 특정 수치의 편중 정도 파악.
> 3. Multiply: Interaction Feature로 시너지 효과 파악. 회귀 분석에 많이 사용함.
> 
> - 분해
> 1. Separate: 특정 변수 활용 기반의 새로운 의미를 파악할 수 있는 특징을 생성. 도메인 지식 및 일반적 개념 기반으로 생성 가능.

### 차원 축소: 원본 데이터로부터 새로운 특징의 집합을 생성. 고차원 원시 데이터 셋을 저차원으로 축소.
> - PCA(Principal Compnent Analysis)
> 1. 변수들이 지닌 정보를 최대한 확보하는 저차원 데이터로 생성.
> 2. 서로 연관된 변수들이 관측되었을 때, 원본 데이터 분산 기반의 특징을 생성함. 
> 3. 주성분 간이 서로 독립을 이루도록함.
> 
> - Featurization via Clustering: 군집 분석 기반의 고차원 데이터를 하나의 특징으로 차원 축소. 
> 1. 이렇게 획득한 군집결과 특징을 분류/회귀 등 문제해결을 위한 입력변수로 활용(Stacking). 
> 2. 여러개의 특징을 하나의 특징으로 축소하여 모델 연산 비용 감소 추구.
> 
> - K-Means 알고리즘: 비슷한 특성을 가진 데이터들을 그룹으로 묶는 군집화 기법. 데이터가 원형으로 뭉쳐져 있는 경우에 잘 작동.
> 1. 초기 중심(Centroid) 무작위로 선택.
> 2. 각 데이터 포인트와 가까운 중심에 데이터 포인트 할당. 유클리드 거리가 주로 사용됨.
> 3. 데이터 포인트들의 평균을 계산하여 새로운 중심을 업데이트.
> 4. 2번과 3번을 데이터 포인터들이 변하지 않을 때까지 반복.
> 5. 데이터 포인터들이 변하지 않으면 알고리즘 종료.

## 12. 데이터 전처리 이해와 실무\_데이터 축소 - 특징 선택
### Feature Selection: 가장 좋은 성능을 보여줄 수 있는 데이터의 Subset(부분 집합)을 찾아내는 방법.
> - 목적
> 1. 특징 생성과는 다르게 원 데이터 공간 내 유의미한 특징을 선택하는 기법.
> 2. 원본 데이터에서 가장 유용한 특징만을 선택.

### 특징 선택 종류
> #### Filter: 특징들에 대한 통계적 점수를 부여하여 순위를 매기고 선택하는 방법론. 실행 속도가 빠르다는 측면에서 시간 및 비용 측면의 장점을 보임.
> - Chi-square filter(카이제곱 필터)
> 1. 범주형인 독립 및 종속 변수 간의 유의미성을 도출하기 위한 통계적 방안.
> 2. 연속형 범주를 이산화하여 활용 가능.
> 3. 과자를 좋아하는 학생들이 과학 실험을 좋아하는지 확인하는 것처럼, 범주형 변수 간의 관계를 확인.
> 
> - Correlation filster
> 1. 연속적인 독립 및 종속변수 간 유의미성을 도출하기 위한 통계적 방안.
> 2. 보통 threshold(임계치) 설정하여 변수 선택.
> 3. 키와 몸무게처럼, 연속적인 변수 간의 관계를 확인.
> 
> #### Wrapper: 특징들의 조합을 지도학습 기반 알고리즘에 반복적으로 적용하여 특징을 선택하는 방법론. 최적의 데이터 조합을 찾기 때문에 성능 관점상 유용하나 시간과 비용이 크게 발생.
> - 반복적 특징 조합 탐색
> 1. 원본 데이터셋 변수들의 다양한 조합을 모델에 적용.
> 2. 최적의 subset을 도출하는 방법론.
> 3. Recursive Feature Elemination이 대표적인 방식.
> 4. 가장 좋은 옷 조합을 찾기 위해 옷장의 옷을 계속 바꿔보는 것처럼, 최적의 특징 조합을 찾음.
> 
> #### Embedded: 모델 정확도에 기여하는 특징들을 선택하는 방법론. Filter와 Wrapper의 장점을 결합. 모델의 학습 및 생성과정에서 최적의 특징을 선택하는 방법.
> - 알고리즘 내 자체 내장 함수로 특징을 선택.
> - 학습과정에서 최적화된 변수를 선택.
> - 트리 계열 모델 기반의 특징 선택이 대표적.

### Boruta
- 특징
> 1. 특성 선택을 위한 랜덤 포레스트 기반 알고리즘. 
> 2. 이 알고리즘은 원래 특성의 중요성을 무작위로 생성된 '그림자' 특성의 중요성과 비교하여 특성의 중요성을 평가. 
> 3. 그림자 특성은 원래 특성의 값을 무작위로 섞은 것으로, 실제로는 중요하지 않아야 함.
> - 단계
> 1. 랜덤 포레스트 모델을 학습하고, 각 특성의 중요도를 계산.
> 2. 원래 특성의 값들을 무작위로 섞어서 그림자 특성을 생성하고, 랜덤 포레스트 모델을 다시 학습.
> 3. 그림자 특성의 최대 중요도보다 원래 특성의 중요도가 높으면, 해당 원래 특성을 중요하다고 판단.
> 4. 이 과정을 여러 번 반복하며, 각 특성이 중요하다고 판단된 횟수를 기록.
> 5. 마지막으로, 특성이 충분히 자주 중요하다고 판단되면, 해당 특성을 선택.

### SVM
> - SVM은 주로 분류와 회귀 문제에 사용되는 강력한 알고리즘.
> - 데이터를 클래스(범주)로 나누는 결정 경계를 찾음.
> - SVM은 고차원 데이터에도 잘 작동하며, 데이터 선택을 통해 중요한 변수들을 찾아낼 수 있음. 
> - 이렇게 선택된 변수들을 사용하면 모델의 복잡성을 줄이고 일반화 성능을 향상시킬 수 있음.

### 특징 선택 알고리즘
> - Boruta Algorithm: 기존 데이터를 임의로 복제하여 랜덤 변수(shadow)를 생성하고 원 자료와 결합하여 랜덤포레스트 모형에 적용함. shoadow 보다 중요도가 낮을 경우 중요하지 않은 변수로 판단 후 제거함. 친구들과 숨바꼭질을 할 때, 진짜 친구와 가짜 친구(인형)를 섞어 놓고, 진짜 친구를 찾는 것처럼 중요한 변수를 찾음.

## 12.5 데이터 탐색 비교
### 일변량 비시각화 탐색
> - 대상: 하나의 변수
> - 방법: 시각화를 사용하지 않는 통계적 방법
> - 예: 평균, 중앙값, 표준편차, 왜도, 첨도 등의 통계치 계산
> - 목적: 해당 변수의 분포, 중심 경향, 변동성 등의 기본 통계적 특성 파악

### 일변량 시각화 탐색
> - 대상: 하나의 변수
> - 방법: 시각화 방법 사용
> - 예: 히스토그램, 박스 플롯, 도수 분포표 등
> - 목적: 데이터의 분포나 이상치 등을 직관적으로 파악

### 다변량 비시각화 탐색
> - 대상: 두 개 이상의 변수
> - 방법: 시각화를 사용하지 않는 통계적 방법
> - 예: 상관 계수, 공분산, 두 변수의 평균 차이 검정 등
> - 목적: 변수 간의 관계나 연관성 파악

### 다변량 시각화 탐색
> - 대상: 두 개 이상의 변수
> - 방법: 시각화 방법 사용
> - 예: 산점도, 히트맵, 버블 차트, 페어 플롯 등
> - 목적: 변수 간의 관계나 패턴, 그룹 내의 차이를 직관적으로 파악

## 13. 데이터 탐색 이해와 실무\_일변량 비시각화 탐색
### EDA(Exploratory Data Analysis): 데이터를 다양한 측면에서 바라보고 이해하는 과정.
> - 개요
> 1. 속성 파악: 분석 목적 및 개별 변수 속성 파악.
> 2. 관계 파악: 변수 간의 관계 파악 및 가설 검증.
>
> - 사전 데이터 탐색
> 1. 데이터 정의 확인.
> 2. 실 데이터 확인: 데이터 개요, 결측치, 형상을 head, tail, info로 확인. 변수별 정의된 범위 빛 분포 등 확인.

## 14. 데이터 탐색 이해와 실무\_일변량 시각화 탐색
### 일변량 비시각화: 분석 대상 데이터가 하나의 변수로 구성되고 요약 통계량, 빈도 등으로 표현하는 탐색 유형.
> - 빈도표: 범주별 빈도 파악이 목적. 범주별 빈도 수 기반의 구성 파악 및 결측치 빈도 파악. 데이터 전체 수 대비 각 범주별 분포 파악.
> - 연속형 비시각화: 기술 통계량(평균, 분산), 사분위수(중앙값), 분포 관련 지표(왜도, 첨도)

### 교차테이블: 교차테이블(cross-tabulation, 또는 crosstab)은 두 개 이상의 범주형 변수의 빈도수를 기록한 테이블.
> - 다차원 빈도수를 정리해서 보여주는 통계학의 기본 도구 중 하나로, 데이터의 패턴을 이해하는 데 유용하게 사용됨.

### 두 범주형 변수 간의 교차 테이블을 생성
> ```python
> crosstab()
> ```

### 파이차트

### 막대그래프: 범주별 비교는 막대 그래프 기반의 파악이 비교적 수월함.

### 히스토그램
> - 연속형 테이터 값들의 분포 파악 가능
> - 구간내 속하는 자료의 수가 많고 적음을 쉽게 파악 가능.

### 커널밀도추청
> - 연속형 데이터 값을의 분포를 분석하여 연속성 있는 확률 밀도 함수를 추정.
> - 변수가 가질 수 있는 모든 값의 확률을 추정하는 것.
> - 히스토그램의 한계점을 극복하기 위해 고안된 방안.

### 박스플롯
> - 연속형 데이터의 양상을 직관적으로 파악할 수 있는 방안으로 5가지 요약치를 기반으로 생성
> 1. 중앙값
> 2. 1분위수
> 3. 3분위수
> 4. 최댓값(IQR Value)
> 5. 최소값
> - 데이터의 개략적인 흩어짐의 형태 파악 및 IQR 기반의 이상치 판단에 용이함

## 15. 데이터 탐색 이해와 실무\_다변량 비시각화 탐색
### 다변량 비시각화: 두개 이상의 변수로 구성된 데이터의 관계를 교차표 및 상관계수 등으로 파악하는 데이터 탐색 유형.

### 범주형 - 범주형
> - 비시각화 방안: Cross tabulation(교차표)
> - 목적
> 1. 두개 범주형 변수의 범주별 연관성 및 구성 파악
> 2. 조합 간 연관 관계 파악

### 범주형 - 연속형
> - 비시각화 방안: 범주별 통계량
> - 목적
> 1. 범주 별 대표 통계량 비교 파악.
> 2. 범주형-연속형 변수 조합 간 볌주 별 대표 수치 비교.

### 연속형 - 연속형
> - 비시각화 방안: Corr.coefficient(상관계수)
> - 목적: 두개 연속형 변수의 관계성 정도 파악.

### 높은 상관계수: 비슷한 정보를 제공하는 밀접한 관계의 변수.
> - 회귀분석에서 독립변수 간에 강한 상관관계 발생으로 인해 다중공선성 발생.
> - 독립변수 간의 관계는 독립적이라는 회귀분석 가정에 위배됨.
> - 회귀 계수가 불안정하여 종속 변수에 미치는 영향력을 올바르게 설명하지 못하므로 모델의 안정성 저해.

## 16. 데이터 탐색 이해와 실무\_다변량 시각화 탐색
### 다변량 시각화: 두개 이상의 변수로 구성된 데이터의 관계를 시각화 기반으로 탐색하는 데이터 탐색 유형

### 범주형 - 범주형
> - 모자이크 플롯
> 1. 두개 범주형 변수 내 범주 별 조합의 빈도 크기를 개략적으로 파악
> 2. 범주 그룹 간 비중의 차이를 전체적으로 파악 가능.
> 3. 범주 수가 많고, 각 조합별 비중 차이가 크지 않을 경우 전체적 파악이 어려울 수 있음.

### 범주형 - 연속형
> - 박스플롯
> 1. 범주 별 기술통계량 및 경향성을 개략적으로 파악.
> 2. 많은 데이터를 눈으로 직접 확인하기 어렵고, 대표적 통계 값만으로 파악하기 어려울때 용이함.
> 3. 범주 그룹(범주형 변수) 간 수치(연속형 변수)의 집합 범위와 중앙값, 이상치 등을 빠르게 확인할 수 있음.
> 4. 비시각화 기반의 단순 수치값 비교보다 데이터가 설명하는 많은 정보 획득 가능.
> 
> - 평행좌표
> 1. 범주형 - 연속형 변수 조합 간 경향성 파악.
> 2. 연속형 데이터 기반으로 범주별 경향석 파악에 용이함.
> 3. 데이터의 트렌드 판단 가능.
> 4. 연속형 변수 간 단위 표준화가 이루어지기 전의 데이터로 시각화할 경우 파악이 어려울 수 있음.

### 연속형 - 연속형
> - 산점도
> 1. 연속형 변수 간 관계성을 개략적으로 파악(선형/비선형 및 음양 방향 등).
> 2. 연속형 데이터 간의 관계를 그래프 상으로 어떠한 관계까 있는지 파악하기 위함.
> 3. 변수 간 분포를 통해 선형 또는 비선형 관계 및 음양의 방향 등을 빠르게 파악할 수 있음.
> 4. 범주 Label 간 비교가 필요할 경우, 해당 부분의 그룹 정보를 표시하면 변수 간 관계 및 범주 그룹 간 관계를 함께 파악 가능.
> 
> - 히트맵(HeatMap): 히트맵은 데이터 매트릭스를 색상으로 표현하여 변수 간의 관계나 데이터의 분포를 시각적으로 파악할 수 있게 해주는 시각화 방법. 
> 1. 대량의 데이터를 한 눈에 파악할 수 있음.
> 2. 변수 간의 관계의 강도를 색의 진하기로 쉽게 파악할 수 있음
> 3. 데이터의 패턴이나 클러스터를 빠르게 확인할 수 있음.

## 17. 데이터 분석 이해와 실무\_가설의 의의와 검정
### 가설 검정: 모집단에 어떤 가설을 설정한 뒤, 통계 기법으로 이용한 가설의 채택 여부를 확률적으로 판정하는 통계적 추론의 방법.

### 귀무가설과 대립가설
> - 귀무가설: 기존 이론(H0).
> - 대립가설: 기존 이론(H1).

### 오류
> - 제1종 오류: 채택된 귀무가설이 잘못되었음(a).
> - 제2종 오류: 채택된 대립가설이 잘못되었음(b).

### 가설 검정 방법: 목적에 맞는 설정 필요.
> - 양측 검정: 검정 통계량에서 분포의 기각 영역이 양쪾에 나타나는 형태의 가설검정.
> - 단측 검정: 검정 통계량에서 분포의 기각 영역이 한쪽에 나타나는 형태의 가설검정.

### 가설 기반 의사 결정 방법
> - 검정 통계량 > 기각역 -> 귀무 가설 기각
> - 검정 통계량 < 기각역 -> 귀무 가설 채택
> - 유의 확률 < 유의수준 -> 귀무 가설 기각
> - 유의 확률 > 유의수준 -> 귀무 가설 채택

### 단일표본 t검정(One-sample t-test): 목적: 단일 모집단의 평균값이 특정 기준값(또는 예상값)과 다른지를 판단하기 위한 검정.
> - 가설 설정
> 1. 귀무가설(H0): 모집단의 평균 = 기준값
> 2. 대립가설(H1): 모집단의 평균 ≠ 기준값

### 독립표본 t검정(Independent two-sample t-test): 두 독립된 집단의 평균값이 서로 다른지를 판단하기 위한 검정.
> - 가설 설정
> 1. 귀무가설 (H0): 집단 A의 평균 = 집단 B의 평균
> 2. 대립가설 (H1): 집단 A의 평균 ≠ 집단 B의 평균
> - 가정: 독립성, 정규성 확인, 등분산성 확인

### ANOVA(분산분석): 세 개 이상의 집단의 평균값이 서로 동일한지를 판단하기 위한 검정.
> - 가설 설정:
> 1. 귀무가설(H0): 모든 집단의 평균은 동일.
> 2. 대립가설(H1): 적어도 한 집단의 평균은 다름.

### 카이제곱 검정(Chi-squared test): 두 범주형 변수의 독립성을 판단하기 위한 검정. 빈도 또는 비율의 차이를 검정하는데 사용됨.
> - 가설 설정
> 1. 귀무가설 (H0): 두 범주형 변수는 독립적임.
> 2. 대립가설 (H1): 두 범주형 변수는 독립적이지 않음.

### 상관분석 (Correlation Analysis): 두 연속형 변수 간의 선형적 관계의 강도와 방향을 측정함.
> - 결과 해석
> 1. 상관계수 값은 -1과 1 사이에 있으며, 0에 가까울수록 두 변수 간의 관계가 약하고, -1 또는 1에 가까울수록 강한 관계를 나타냄.
> 2. 부호는 방향성을 나타냅니다. 양의 상관은 한 변수가 증가할 때 다른 변수도 증가하는 경향을, 음의 상관은 한 변수가 증가할 때 다른 변수가 감소하는 경향을 나타냄.

### 가설 점정 순서
> 1. 가설 수립
> 2. 판단 기준 수립
> 3. 통계 기법 도출
> 4. 분석 통계량 산출
> 5. 판단 기준
> 6. 결과 도출

## 18. 데이터 분석 이해와 실무\_상관 분석
### 상관관계: 변수 간의 상호 관련성을 의미하며, 관계성의 정도는 통계적 또는 시각적인 방법으로 파악 가능.

### 산점도: 산점도를 이용하면 상관관계를 쉽게 파악 가능.
> - 두 연속형 데이터의 관계 파악에 용이.
> - 특정 관계를 갖고 있는 데이터는 한 눈에 파악 가능. 
> - 극단적 혹은 이상치 파악 가능.
> - 한 변수의 값이 증가할 때, 다른 변수의 값도 같이 증가한다면 두 변수는 양의 상관관계.
> - 한 변수는 증가하고 다른 변수는 감소한다면, 두 변수는 음의 상관관계.

### 상관분석: 측정된 두 변수 간의 선형 관계까 있는지 탐색 및 확인하는 분석 방법.
- 피어슨 상관분석
> 1. 귀무가설: 선형관계 없음.
> 2. 대립가설: 선형관계 있음.
> 
> - 기본 가정
> 1. 선형성: 두 변수의 관계는 선형적.
> 2. 정규성: 집단별 종속변수는 정규분포를 만족.
> 3. 등분산성: 집단별 종속변수 분포의 분산은 동일.
> 
> - 주의점
> 1. 상관관계가 있다고 인과관계가 있는 것은 아님.
> 2. 인과관계가 있으면 상관관계는 있음.

### 상관분석 프로세스
> 1. Data 특성 파악 및 가설 설정: 종속변수와 독립변수는 모두 등비여야 함.
> 2. 산점도를 이용한 시각화 및 검토: 변수별 정규성, 등분산성 체크.
> 3. 목적에 맞는 분석 수행: 최종 분석 방법 선택.
> 4. 결과 해석 및 결론 도출: P값 기반 상관관계 변수 선택. 관계의 정도 확인.

### 상관계수: 두 변수 간의 함께 변화하는 경향을 객관적으로 측정할 수 있는 척도.
> - 피어슨 상관계수: 선형관계의 강도를 측정. 상관계수 r은 -1부터 1까지 값을 가짐
> 1. r=0: 두 변수간 선형관계가 없음.
> 2. r=1: 완벽한 우상향 직선의 관계. 양의 상관관계.
> 3. r=-1: 완벽한 우하향 직선의 관계. 음의 상관관계.

> - 스피어맨 상관계수: 비선형 순위 상관관계를 측정.
> 1. 스피어맨은 정규분포가 아니어도 monotonic(단조) 증가/하락에 관한 비선형 관계 포함 가능.

## 19. 데이터 분석 이해와 실무\_회귀 분석
### 단순회귀분석: 한 개의 종속변수(Y)와 한 개의 독립변수(X) 사이의 관계를 분석하는 통계 기법.
> - 기본 가정
> 1. 선형성: 독립변수(X)와 종속변수(Y)는 선형관계임.
> 2. 독립성: 종속변수 Y는 서로 독립이어야 함.
> 3. 등분산성: 독립변수 X의 값에 관계없이 종속변수 Y의 분산은 일정함.
> 4. 정규성: 독립변수 X의 고정된 어떤 값에 대하여 종속변수 Y는 정규 분포를 따름.
> 
> - 최소제곱법(OLS): 잔차를 최소화하는 회귀 계수 추청.
> 
> - 결정계수
> 1. 전체 제곱합(SST) = 회귀 제곱합(SSR) + 잔차 제곱합(SSE)
> 2. 총 변동을 설명하는데 있어서 회귀선에 의하여 설명되는 변동 기여 비율.
> 3. t검정: 단순회귀계수를 검정할 때, 개별회귀계수의 통계적 유의성은 t검정으로 확인.

### 다중회귀분석: 단순회귀분석의 확장으로 독립변수가 두 개 이상인 회구모형에 대한 분석.
> - 다중선형회귀모델
> - 단순회귀와의 차이점: 단일 개의 독립변수가 아닌 여러 개의 독립 변수를 사용.
> - 다중공선성
> 1. 독립변수 간 독립성이 없는 상관성 존재를 의미.
> 2. 여러 개의 독립 변수가 존재할 때 종속변수의 영향을 주는 독립변수를 찾는 것이 중요하며, 최적의 변수 선택 필요.

### 이차회귀모델: 비선형성을 고려한 이차회귀분석.

### 다항회귀모델
> - 2차 이상의 회귀모형.
> - 변수 간 상호작용 가능.
> - 비선형적 추세를 고려할 수 있음.

## 20. 데이터 분석 이해와 실무\_시계열 분석
### 시계열 데이터: 순차적인 시간의 흐름에 따라 기록된 데이터.
> - 구성요소
> 1. 추세(Trend): 시간의 흐름에 따라 점진적이고 지속적인 변화.
> 2. 계절성(Seasonality): 특정 주기에 따라 일정한 패턴을 갖는 변화.
> 3. 싸이클(Cycle): 경제 또는 사회적 요인에 의한 변화. 일정 주기가 없는 장기적인 변화.
> 4. 잔차(Residuals): 설명할 수 없는 변화.
> 
> - 특징
> 1. 현재 시점의 시계열 데이터를 분석하는데 이전 시간의 값이 현재에도 영향을 끼칠 것이라는 가정하에 회귀분석을 진행.
> 2. 자기 상관(Autucorrelation) 존재.
> 3. 대표적으로 자기회귀, 이동평균, 자기회구누적이동평균, 벡터자기회귀 모델 등이 존재.
> 4. 현재 시점에 가까운 데이터일수록 서로 강한 관계를 맺는 경향이 존재.

### 시계열 분석 순서
> 1. 원계열 시도표 확인
> 2. 추세 및 계절성 분리
> 3. 잔차 확인
> 4. 잔차가 백색 잡음인지의 여부를 확인
> 5. 모델 선택
> 6. 예측 결과 확인 및 해석

### 정상성: 정상성을 나타내는 시계열은 관측치가 시관과 무관하여야 함.

### 모델
> - 자기회귀 모델(AR): 시계열의 미래 값이 과거 값에 기반한다는 모델. 이전 값의 영향을 받음.
> 1. 기본 아이디어: 과거의 데이터 포인트들이 미래의 데이터 포인트에 영향을 줌.
> 2. 예시: "어제 비가 왔다면, 오늘도 비가 올 확률이 높다"라는 개념.
> 
> - 이동평균 모델(MA): 전체적인 편향성을 다루는 모델. 설명변수가 최근 오차항으로만 구성되어 있음. 이전 시점의 예측 오차에 가중치를 두어 미래의 값을 예측.
> 1. 기본 아이디어: 현재의 데이터 포인트는 최근의 오차(또는 '충격')들에 의해 영향을 받음.
> 2. 예시: "어제의 날씨 예측이 얼마나 틀렸는지"가 "오늘의 날씨가 어떨지"에 영향을 줌
> 
> - ARIMA 모델: AR과 MA를 고려하고, 누적(Integretion)으로 추세를 고려한 모델. 자기회귀 누적 이동평균 모델.
> 1. 기본 아이디어: AR과 MA 모델의 아이디어를 결합하고, 또한 누적된 데이터(또는 추세)를 포함하여 더 복잡한 시계열 패턴을 모델링.
> 2. 예시: "어제 비가 왔고, 어제의 날씨 예측이 얼마나 틀렸는지, 그리고 최근 몇 년 동안의 비의 추세"가 "오늘의 날씨가 어떨지"에 영향을 준줌.
> 
> - ACF(Autocorrelation Function)
> 1. ACF는 시계열 데이터에서 한 시점의 값과 다른 시점의 값 사이의 상관 관계를 측정.
> 2. 특히 ACF는 특정 시차(lag)에 대해 직접적인 상관 관계를 나타냄.
> 3. 이를 통해 시계열 데이터의 패턴, 주기성 및 추세를 파악할 수 있음.
> 
> - PACF(Partial Autocorrelation Function)
> 1. PACF는 다른 시차의 영향을 배제하고 두 시점의 값 사이의 상관 관계를 측정.
> 2. 예를 들어, PACF의 2차 시차는 1차 시차의 영향을 배제하고 2차 시차만의 상관 관계를 나타냄.
> 3. ARIMA와 같은 시계열 모델의 파라미터를 결정하는 데 도움을 줌.

## 21. 데이터 시각화 이해와 실무\_정적 시각화 개요
### 데이터 시각화: 직관적으로 정보를 확인하는 효과적인 방법.

### 그래프 구성 요소
> - figure: 도화지.
> - axes: 도화지 내 plot이 그려지는 공간.
> - axis

## 22. 데이터 시각화 이해와 실무\_정적 시각화 실습 - matplotlib 활용

### Matplotlib: 데이터 시각화 라이브러리. 2D 형태의 그래프와 이미지를 그릴 때 많이 사용.
> #### 그래프 그리기
> 1. plt.figure()
> 2. plt.plot(): 데이터 시각화 기능 담당.
> 3. plt.show()

### plt.subplot(row, column, index)
> - 각 subplot에 다른 종류의 그래프(예: 선 그래프, 막대 그래프)를 그릴 때 효과적.

### subplots: axes 객체의 twinx메소드를 이용하면 x축을 공유하는 2개의 그래프를 통시에 그릴 수 있음.
> - 동일한 유형의 그래프를 여러 개 그릴 때, 또는 그래프 전체에 대한 제어가 필요할 때 효과적.

## 23. 데이터 시각화 이해와 실무\_정적 시각화 실습 - seaborn 활용
### Seaborn: Matplotlib을 기반으로 하며 디자인 테마와 통계용 차트가 추가된 시각화 라이브러리.
> - 변수가 추가될 수록 메모리 부족 이슈 및 가독성이 떨어짐.
> - 고수준 시각화에 특화: 산점도 구현, 추세선 출력, jointplot을 포함한 여러 플롯, statsmodels를 이용한 데이터 분포 시각화.
> - 다채로운 시각화 기능을 제공.

### scatterplot: 두 변수 간의 관계를 나타내는 산점도.
> - regplot: 산점도와 선형 회귀 모델의 선을 함께 그림.
> 1. 선형 회귀선을 그리는 기본 기능을 제공하며, 더 단순하고 유연함.
> 2. matplotlib의 축 객체에 그릴 수 있어서, 다중 서브플롯 등에서 사용하기 좋음.

### rugplot: 단변량 분포를 1차원으로 나타냄.
> - 단변량 분포:
> 1. 하나의 변수에 대한 데이터의 분포를 나타냄.
> 2. 단변량 분포에서는 해당 변수의 중심 경향, 퍼짐, 왜도 등을 살펴볼 수 있음.
> 3. rugplot은 이러한 단변량 분포를 시각적으로 나타내주어, 데이터가 어디에 얼마나 집중되어 있는지 살펴보는 데 유용함.

### lmplot: regplot과 비슷하지만, 더 복잡한 모델을 만드는 데 유용함.
> - regplot과 기본 기능은 유사하나, FacetGrid를 사용하여 추가적인 카테고리 변수를 적용할 수 있음.
> - hue, col, row 등의 매개변수를 사용해 복잡한 모델을 표현할 수 있어 다변량 데이터를 분석하기에 좋음.
> - 상대적으로 더 높은 수준의 인터페이스를 제공함.

### pairplot: 데이터셋 내의 여러 변수 간의 관계를 시각화. 전체 판에 그려짐.

### pivot_table: pivot_table 메서드를 사용하여 피벗 테이블을 만듬.

### heatmap: 데이터의 밀도나 빈도를 시각화.

## 24. 데이터 시각화 이해와 실무\_동적 시각화 개요
### 동적 시각화: 유저와 시각화 결과의 상호작용으로 다양한 정보를 즉각적으로 전달.
> - 대시보드
> 1. 하나의 화면에서 여러 정보가 연동.
> 2. 사용자 대화형 특성.
> 3. 업무에 필요한 데이터를 실시간으로 함축하여 전달.
>
> - plotly
> 1. express: 쉬움. 유연성이 떨어짐.
> 2. graph_object: 원하는 기능 구현 가능.

## 25. 데이터 시각화 이해와 실무\_동적 시각화 실습 - plotly 활용
### graph_objects: 데이터 시각화 모듈.
> - 과정
> 1. fig = go.Figure()
> 2. fig.add_traces(graph)
> 3. fig.update_layout(option)

### Candlestick 차트: 주로 주식의 시가(Open), 최고가(High), 최저가(Low), 종가(Close) 데이터를 사용.

## 26. 데이터 시각화 이해와 실무\_UI 시각화 실습 - streamlit 활용
### Streamlit: Data Science를 위한 맞춤형 웹 애플리케이션을 만들 수 있는 파이썬의 라이브러리.
> - 구성요소: 텍스트 요소, 인풋 위젯, 이미지 및 상태바
> - 실행 방법
> ```shell
> streamlit run [file_name]
> ```