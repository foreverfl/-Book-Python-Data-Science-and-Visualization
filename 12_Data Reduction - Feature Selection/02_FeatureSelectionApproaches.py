"""
- 특징 선택 종류
1. Filter: 특징들에 대한 통계적 점수를 부여하여 순위를 매기고 선택하는 방법론. 실행 속도가 빠르다는 측면에서 시간 및 비용 측면의 장점을 보임.
* Chi-square filter(카이제곱 필터): 범주형인 독립 및 종속 변수 간의 유의미성을 도출하기 위한 통계적 방안. 연속형 범주를 이산화하여 활용 가능.
* Correlation filster: 연속적인 독립 및 종속변수 간 유의미성을 도출하기 위한 통계적 방안. 보통 threshold(임계치) 설정하여 변수 선택.
2. Wrapper: 특징들의 조합을 지도학습 기반 알고리즘에 반복적으로 적용하여 특징을 선택하는 방법론. 최적의 데이터 조합을 찾기 때문에 성능 관점상 유용하나 시간과 비용이 크게 발생.
* 반복적 특징 조합 탐색: 원본 데이터셋 변수들의 다양한 조합을 모델에 적용. 최적의 subset을 도출하는 방법론. Recursive Feature Elemination이 대표적인 방식.
3. Embedded: 모델 정확도에 기여하는 특징들을 선택하는 방법론. Filter와 Wrapper의 장점을 결합. 모델의 학습 및 생성과정에서 최적의 특징을 선택하는 방법.
* 알고리즘 내 자체 내장 함수로 특징을 선택. 학습과정에서 최적화된 변수를 선택. 트리 계열 모델 기반의 특징 선택이 대표적.

- 특징 선택 알고리즘
* Boruta Algorithm: 기존 데이터를 임의로 복제하여 랜덤 변수(shadow)를 생성하고 원 자료와 결합하여 랜덤포레스트 모형에 적용함. shoadow 보다 중요도가 낮을 경우 중요하지 않은 변수로 판단 후 제거함.
"""
